{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Day Analysis\n",
    "\n",
    "This is a sketch of how to do a single day analysis using pyspark mainly based on the scala source code\n",
    "\n",
    "### Libraries\n",
    "\n",
    "Sources:\n",
    "\n",
    "https://medium.com/@mrpowers/chaining-custom-pyspark-transformations-4f38a8c7ae55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "#from pyspark.sql import Row\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "# This is a patch on the pyspark DataFrame to support transform similar to scala\n",
    "def transform(self, f):\n",
    "    return f(self)\n",
    "\n",
    "DataFrame.transform = transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PWS Information and Globals\n",
    "\n",
    "This is the infromation of the sensor that performed the measurements. The ID is the one given by wunderground. In this case we use only cinvestav telchac."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PWS info\n",
    "pwsID = 'IYUCATNT2'\n",
    "pwsTz = 'America/Merida'\n",
    "\n",
    "FIVE_MINUTES_IN_SECS = (5*60) # Seconds\n",
    "TWO_HOURS_IN_5MINS = (2*60)//5 # This iw hos many 5 min periods there is in 2 hours in \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-14dac852959f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Changing the default timezone for the analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.session.timeZone\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwsTz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Inferr schema creates a problem when changing the session timezone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#    .option(\"inferSchema\",\"true\") \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Changing the default timezone for the analysis\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", pwsTz)\n",
    "\n",
    "# Inferr schema creates a problem when changing the session timezone\n",
    "#    .option(\"inferSchema\",\"true\") \\\n",
    "    \n",
    "staticDataFrame = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .load(\"12/*.csv\")\n",
    "    #.load(\"IYUCATNT2-2016-04-18.csv\")\n",
    "    #.load(\"mini.csv\")\n",
    "\n",
    "staticDataFrame.printSchema()\n",
    "staticDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Timestamp and Wind\n",
    "\n",
    "Extracting only wind and time information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windDF = staticDataFrame.selectExpr(\n",
    "    \"Time\",\n",
    "    \"to_timestamp(Time,'yyyy-MM-dd HH:mm:ss') as TS\",\n",
    "    \"cast(WindDirectionDegrees as integer)\",\n",
    "    \"cast(WindSpeedMPH as double)\") \\\n",
    "    .withColumn(\"Date\",col(\"TS\").cast(\"date\")) \\\n",
    "    .withColumn(\"TSEpochSec\",col(\"TS\").cast(\"long\"))\n",
    "    #\"cast(WindSpeedGustMPH as double)\")\n",
    "# Creating SQL for future analisys\n",
    "\n",
    "windDF.printSchema()\n",
    "windDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling Using Interpolation\n",
    "\n",
    "We need to a ply a window function to get the next reading together with the current one in order to interpolate values between them.\n",
    "\n",
    "Drop the last row with a null lead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"Date\").orderBy(\"TS\")\n",
    "\n",
    "windPairDF = windDF \\\n",
    "    .withColumn(\"nextWindDirectionDegrees\",lead(\"WindDirectionDegrees\").over(windowSpec)) \\\n",
    "    .withColumn(\"nextWindSpeedMPH\",lead(\"WindSpeedMPH\").over(windowSpec)) \\\n",
    "    .withColumn(\"nextTSEpochSec\",lead(\"TSEpochSec\").over(windowSpec)) \\\n",
    "    .dropna(subset=[\"nextTSEpochSec\"])\n",
    "\n",
    "#||      Date|TSEpochSec\n",
    "\n",
    "#windPairDF.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correction of the Next Wind Direction\n",
    "\n",
    "In case of boundary conditions between 0 and 360 degrees representing the north we will have a problem when doing interpolation. For instance interpolating values between 5 degrees and 355 degrees may take values between these twon numbers which will be incorrect. To overcome this problem we have to calculate what is the shortest angle separation between those 2 readings and then calculate the new next direction based on this.\n",
    "\n",
    "The one liner for the shortest angle was obtained from a discussion in stack overflow:\n",
    "\n",
    "Source:\n",
    "https://stackoverflow.com/questions/2708476/rotation-interpolation/14498790#14498790"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortestAngleExpr(fromAngle, toAngle) :\n",
    "    return ( ( ( ( (toAngle-fromAngle) % lit(360) ) + lit(540) ) % lit(360) ) - lit(180) )\n",
    "\n",
    "def shortestNextAngleExpr(fromAngle, toAngle) :\n",
    "    return fromAngle + shortestAngleExpr(fromAngle, toAngle)\n",
    "\n",
    "windPairFixDirDF = windPairDF \\\n",
    "    .withColumn(\"oldNextWindDeg\",\n",
    "                col(\"nextWindDirectionDegrees\")) \\\n",
    "    .withColumn(\"nextWindDirectionDegrees\",\n",
    "                shortestNextAngleExpr(col(\"WindDirectionDegrees\"), col(\"nextWindDirectionDegrees\")))\n",
    "\n",
    "\n",
    "#windPairFixDirDF = windPairDF \\\n",
    "#    .withColumn(\"shortestAngle\",shortestAngleExpr(col(\"WindDirectionDegrees\"), col(\"nextWindDirectionDegrees\"))) \\\n",
    "#    .withColumn(\"neShortWindDirectionDegrees\",expr(\"WindDirectionDegrees+shortestAngle\"))\n",
    "\n",
    "#windPairFixDirDF = windPairDF \\\n",
    "#    .withColumn(\"shortestAngle\",expr(\"((((nextWindDirectionDegrees-WindDirectionDegrees)%360)+540)%360)-180\")) \\\n",
    "#    .withColumn(\"neShortWindDirectionDegrees\",expr(\"WindDirectionDegrees+shortestAngle\"))\n",
    "\n",
    "windPairFixDirDF.show()\n",
    "# For reference steps:\n",
    "#    .withColumn(\"diff\",expr(\"nextWindDirectionDegrees-WindDirectionDegrees\")) \\\n",
    "#    .withColumn(\"mod\",expr(\"(nextWindDirectionDegrees-WindDirectionDegrees)%360\")) \\\n",
    "#    .withColumn(\"p540\",expr(\"((nextWindDirectionDegrees-WindDirectionDegrees)%360)+540\")) \\\n",
    "#    .withColumn(\"mod2\",expr(\"(((nextWindDirectionDegrees-WindDirectionDegrees)%360)+540)%360\")) \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Slopes and Intercept for Linear Equations\n",
    "\n",
    "For linear interpolation we have to get a line:\n",
    "\n",
    "$$f(t) = m*t + b$$\n",
    "\n",
    "We need to calculate those values given the current and next (reading,timestamp) pair\n",
    "\n",
    "\"\"\"\n",
    "    .withColumn(\"m\",\n",
    "      expr(\"nextWindSpeedMPH-WindSpeedMPH\")\n",
    "        /(col(\"nextTSEpochSec\").cast(\"long\")-col(\"TSEpochSec\").cast(\"long\"))\n",
    "    ) \\\n",
    "    .withColumn(\"b\",\n",
    "      (col(\"TSEpochSec\").cast(\"long\")*col(\"nextWindSpeedMPH\")-col(\"nextTSEpochSec\").cast(\"long\")*col(\"WindSpeedMPH\"))\n",
    "        /(col(\"TSEpochSec\").cast(\"long\")-col(\"nextTSEpochSec\").cast(\"long\"))\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Column functions to calculate slope and intercept given 2 points:\n",
    "def slopeExpr(x1,y1,x2,y2) :\n",
    "    return (\n",
    "        (y2-y1) \n",
    "        /(x2-x1)\n",
    "    )\n",
    "\n",
    "def interceptExpr(x1,y1,x2,y2) :\n",
    "    return (\n",
    "      (x1*y2-x2*y1)\n",
    "        /(x1-x2)\n",
    "    )\n",
    "\n",
    "def linearParamTrans(srcDF, paramColName, indColName) :\n",
    "    \"\"\"Transformation to create linear parameters for a set of colums.\n",
    "\n",
    "    The 2nd and 3rd parameters are strings that are going to be used to\n",
    "    generate column names in this transformation.\n",
    "\n",
    "    Args:\n",
    "        srcDF (DataFrame) : The pyspark DataFrame to apply the transformation.\n",
    "        paramColName (str): Base name of the parameter we want to get the linear coeff.\n",
    "        indColName (str)  : Base name of the independent variable.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with m{paramColName} b{paramColName} columns\n",
    "    \"\"\"\n",
    "    return srcDF \\\n",
    "        .withColumn(\"m\"+paramColName,\n",
    "                    slopeExpr(col(indColName),col(paramColName),\n",
    "                              col(\"next\"+indColName),col(\"next\"+paramColName))) \\\n",
    "        .withColumn(\"b\"+paramColName,\n",
    "                    interceptExpr(col(indColName),col(paramColName),\n",
    "                                  col(\"next\"+indColName),col(\"next\"+paramColName))) \\\n",
    "\n",
    "\n",
    "#windLinearDF = linearParamTrans(windPairFixDirDF,\"WindSpeedMPH\",\"TSEpochSec\")\n",
    "\n",
    "windLinearDF = windPairFixDirDF \\\n",
    "    .transform(lambda df : linearParamTrans(df,\"WindSpeedMPH\",\"TSEpochSec\")) \\\n",
    "    .transform(lambda df : linearParamTrans(df,\"WindDirectionDegrees\",\"TSEpochSec\"))\n",
    "    \n",
    "#windLinearDF = windPairFixDirDF \\\n",
    "#    .withColumn(\"mWindSpeedMPH\",\n",
    "#                slopeExpr(col(\"TSEpochSec\"),col(\"WindSpeedMPH\"),col(\"nextTSEpochSec\"),col(\"nextWindSpeedMPH\"))) \\\n",
    "#    .withColumn(\"bWindSpeedMPH\",\n",
    "#                interceptExpr(col(\"TSEpochSec\"),col(\"WindSpeedMPH\"),col(\"nextTSEpochSec\"),col(\"nextWindSpeedMPH\"))) \\\n",
    "#    .withColumn(\"mWindDirectionDegrees\",\n",
    "#                slopeExpr(col(\"TSEpochSec\"),col(\"WindDirectionDegrees\"),col(\"nextTSEpochSec\"),col(\"nextWindDirectionDegrees\"))) \\\n",
    "#    .withColumn(\"bWindDirectionDegrees\",\n",
    "#                interceptExpr(col(\"TSEpochSec\"),col(\"WindDirectionDegrees\"),col(\"nextTSEpochSec\"),col(\"nextWindDirectionDegrees\"))) \\\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    .withColumn(\"m\",\n",
    "      (col(\"nextWindSpeedMPH\")-col(\"WindSpeedMPH\"))\n",
    "        /(col(\"nextTSEpochSec\")-col(\"TSEpochSec\"))\n",
    "    ) \\\n",
    "    .withColumn(\"b\",\n",
    "      (col(\"TSEpochSec\")*col(\"nextWindSpeedMPH\")-col(\"nextTSEpochSec\")*col(\"WindSpeedMPH\"))\n",
    "        /(col(\"TSEpochSec\")-col(\"nextTSEpochSec\"))\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    .withColumn(\"m\", lit(0)) \\\n",
    "    .withColumn(\"b\",col(\"WindSpeedMPH\"))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    .withColumn(\"m\",\n",
    "      expr(\"nextWindSpeedMPH-WindSpeedMPH\")\n",
    "        /(col(\"nextTSEpochSec\").cast(\"long\")-col(\"TSEpochSec\").cast(\"long\"))\n",
    "    ) \\\n",
    "    .withColumn(\"b\",\n",
    "      (col(\"TSEpochSec\").cast(\"long\")*col(\"nextWindSpeedMPH\")-col(\"nextTSEpochSec\").cast(\"long\")*col(\"WindSpeedMPH\"))\n",
    "        /(col(\"TSEpochSec\").cast(\"long\")-col(\"nextTSEpochSec\").cast(\"long\"))\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "windLinearDF.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(spark.version) >= '2.13.1'\n",
    "#from distutils.version import StrictVersion\n",
    "#StrictVersion(spark.version) >= StrictVersion('2.4.0')\n",
    "#(spark.version) >= '2.13.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range UDF to Create The Resampling Points\n",
    "\n",
    "We will need help from a UDF and \n",
    "\n",
    "UDF version is more forgivin with bad ranges. For instance if the end of the range is smaller than the start the native spark function will fail:\n",
    "\n",
    "java.lang.IllegalArgumentException: Illegal sequence boundaries: 1483178400 to 1483178399 by 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.version import StrictVersion\n",
    "from pyspark.sql.types import ArrayType, LongType\n",
    "\n",
    "# To be more consistent with spark 2.4 the new version of the UDF is just a wrapper for range in python\n",
    "def rangeWrapper(start, end, step) :\n",
    "    # In python the end is not included in the range, in spark/scala it is\n",
    "    return list(range(start,end+1,step))\n",
    "\n",
    "sequenceUdf = udf(rangeWrapper,ArrayType(LongType()))\n",
    "\n",
    "#Column functions required\n",
    "\n",
    "def rangeStartExpr(start,step) :\n",
    "    return (((start+step-lit(1))/step).cast(\"long\")*step)\n",
    "\n",
    "def rangeEndExpr(end) :\n",
    "    return (end-lit(1))\n",
    "\n",
    "# Wrapper for spark version UDF vs native\n",
    "def sequenceVer(start, end, step) :\n",
    "    if StrictVersion(spark.version) >= StrictVersion('2.4.0') :\n",
    "        return sequence(start, end, step)\n",
    "    else :\n",
    "        return sequenceUdf(start, end, step)\n",
    "\n",
    "#FIVE_MINUTES_IN_SECS = (5*60) # Seconds\n",
    "\n",
    "arrDF = windLinearDF \\\n",
    "    .withColumn(\"range\",\n",
    "                sequenceVer(\n",
    "                    rangeStartExpr(col(\"TSEpochSec\"),lit(FIVE_MINUTES_IN_SECS)),\n",
    "                    rangeEndExpr(col(\"nextTSEpochSec\")),\n",
    "                    lit(FIVE_MINUTES_IN_SECS)))\n",
    "\n",
    "arrDF.printSchema()\n",
    "arrDF.show(100)\n",
    "expDF = arrDF \\\n",
    "    .withColumn(\"interTSEpochSec\",explode(col(\"range\"))) #\\\n",
    "\n",
    "\"\"\"\n",
    "# Test of converged version:\n",
    "testDF = windLinearDF.select(\n",
    "    col(\"TS\"),\n",
    "    col(\"TSEpochSec\"),\n",
    "    col(\"nextTSEpochSec\"),\n",
    "    FIVE_MINUTES_IN_SECS,\n",
    "    (((col(\"TSEpochSec\")+FIVE_MINUTES_IN_SECS-1)/FIVE_MINUTES_IN_SECS).cast(\"long\")*FIVE_MINUTES_IN_SECS).alias(\"newStart\"),\n",
    "    (rangeStartExpr(col(\"TSEpochSec\"),FIVE_MINUTES_IN_SECS)).alias(\"newXStart\"),\n",
    "    (col(\"nextTSEpochSec\")-lit(1)).alias(\"newEnd\"),\n",
    "    (rangeEndExpr(col(\"nextTSEpochSec\"))).alias(\"newXEnd\")\n",
    ").withColumn(\"seqUdf\",sequenceUdf(col(\"newStart\"),col(\"newEnd\"),FIVE_MINUTES_IN_SECS)) \\\n",
    ".withColumn(\"seq\",sequence(col(\"newStart\"),col(\"newEnd\"),FIVE_MINUTES_IN_SECS)) \\\n",
    ".withColumn(\"seqXUdf\",sequenceUdf(\n",
    "    rangeStartExpr(col(\"TSEpochSec\"),FIVE_MINUTES_IN_SECS),\n",
    "    rangeEndExpr(col(\"nextTSEpochSec\")),\n",
    "    FIVE_MINUTES_IN_SECS)) \\\n",
    ".withColumn(\"seqX\",sequence(\n",
    "    rangeStartExpr(col(\"TSEpochSec\"),FIVE_MINUTES_IN_SECS),\n",
    "    rangeEndExpr(col(\"nextTSEpochSec\")),\n",
    "    FIVE_MINUTES_IN_SECS)) \n",
    "\n",
    "testDF.show()\n",
    "#arrDF = windLinearDF.withColumn(\"range\",epoch5MinRangeUdf(col(\"TSEpochSec\"),col(\"nextTSEpochSec\")))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql.types import ArrayType, LongType\n",
    "#def inter5Range(tsX:Timestamp,tsY:Timestamp) : Seq[Timestamp] = {\n",
    "  #  val FIVE_MINUTES_IN_MILLIS : Long = 5*60*1000;//millisecs\n",
    "  #  val x = tsX.getTime()\n",
    "  #  val y = tsY.getTime()\n",
    "  #  (((x+FIVE_MINUTES_IN_MILLIS-1)/FIVE_MINUTES_IN_MILLIS)*FIVE_MINUTES_IN_MILLIS to (y-1) by FIVE_MINUTES_IN_MILLIS).map(new Timestamp(_))\n",
    "  #}\n",
    "\n",
    "def epoch5MinRange(epochSecStart, epochSecEnd) :\n",
    "    FIVE_MINUTES_IN_SECS = 5*60 # Seconds\n",
    "    # range doesn't include the end\n",
    "    #[x for x in range( ((start+five-1)//five)*five ,end,five)]\n",
    "    #return (epochSecEnd-epochSecStart)\n",
    "    #return 100\n",
    "    # Returning a list is required in python3 otherwise we don;t return an array\n",
    "    return list(range(\n",
    "        ((epochSecStart+FIVE_MINUTES_IN_SECS-1)//FIVE_MINUTES_IN_SECS)*FIVE_MINUTES_IN_SECS,\n",
    "        epochSecEnd,\n",
    "        FIVE_MINUTES_IN_SECS))\n",
    "\n",
    "epoch5MinRangeUdf = udf(epoch5MinRange,ArrayType(LongType()))\n",
    "#epoch5MinRangeUdf = udf(epoch5MinRange)\n",
    "\n",
    "\n",
    "#list(epoch5MinRange(1483164120,1483164720))\n",
    "\n",
    "arrDF = windLinearDF.withColumn(\"range\",epoch5MinRangeUdf(col(\"TSEpochSec\"),col(\"nextTSEpochSec\")))\n",
    "#arrDF.printSchema()\n",
    "#arrDF.show()\n",
    "expDF = arrDF \\\n",
    "    .withColumn(\"interTSEpochSec\",explode(col(\"range\"))) #\\\n",
    "#    .withColumn(\"interWindSpeedMPH\",expr(\"(interTSEpochSec * mWindSpeedMPH) + bWindSpeedMPH\"))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolating Resampling Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearEquationExpr(x,m,b) :\n",
    "    return (m*x + b)\n",
    "\n",
    "def linearInterpolationTrans(srcDF, paramColName, indColName) :\n",
    "    \"\"\"Transformation to interpolate values.\n",
    "\n",
    "    The 2nd and 3rd parameters are strings that are going to be used to\n",
    "    generate column names in this transformation.\n",
    "\n",
    "    Args:\n",
    "        srcDF (DataFrame) : The pyspark DataFrame to apply the transformation.\n",
    "        paramColName (str): Base name of the parameter we want to interpolate.\n",
    "        indColName (str)  : Base name of the independent variable.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with inter{paramColName} column\n",
    "    \"\"\"\n",
    "    return srcDF \\\n",
    "        .withColumn(\"inter\"+paramColName,\n",
    "                    linearEquationExpr(col(indColName), col(\"m\"+paramColName), col(\"b\"+paramColName)))\n",
    "\n",
    "\n",
    "interDF = expDF \\\n",
    "    .transform(lambda df : linearInterpolationTrans(df,\"WindSpeedMPH\",\"interTSEpochSec\")) \\\n",
    "    .transform(lambda df : linearInterpolationTrans(df,\"WindDirectionDegrees\",\"interTSEpochSec\"))\n",
    "\n",
    "#    .withColumn(\"interWindSpeedMPH\",linearEquationExpr(col(\"interTSEpochSec\"), col(\"mWindSpeedMPH\"), col(\"bWindSpeedMPH\")))\n",
    "#    .withColumn(\"interWindSpeedMPH\",expr(\"(interTSEpochSec * mWindSpeedMPH) + bWindSpeedMPH\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract and Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#expDF.show()\n",
    "#windLinearDF.withColumn(\"range\",(col(\"TSEpochSec\")-col(\"nextTSEpochSec\"))).show()\n",
    "\n",
    "#cleanDF = expDF.selectExpr(\"Time\",\"TS\",\"cast(TSEpochSec as timestamp)\")\n",
    "#cleanDF.printSchema()\n",
    "#cleanDF.show(100)\n",
    "\n",
    "#cleanDF = expDF.selectExpr(\"Time\",\"TS\",\"cast(TSEpochSec as timestamp)\")\n",
    "\n",
    "#windExtractDF = expDF.selectExpr(\"Date as LocalDate\",\"cast(interTSEpochSec as timestamp) AS Time\",\"interWindSpeedMPH\",\"TS\",\"WindSpeedMPH\")\n",
    "\n",
    "windExtractDF = interDF.selectExpr(\"Date as LocalDate\",\"cast(interTSEpochSec as timestamp) AS Time\",\"interWindSpeedMPH\")\n",
    "windExtractDF.show(100)\n",
    "\n",
    "#windExtractDF.coalesce(1).write.option(\"header\",\"true\").csv(\"output\")\n",
    "windExtractDF.sort(\"Time\").coalesce(1).write.option(\"header\",\"true\").csv(\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a table that will be used for analysis from now on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interDF.show()\n",
    "\n",
    "#windDF = interDF\n",
    "#windDF.createOrReplaceTempView(\"windTable\")\n",
    "\n",
    "# Inter variables now become the main variables\n",
    "#Date|interTSEpochSec|interWindSpeedMPH|interWindDirectionDegrees|\n",
    "\n",
    "# Examples of cleanup on the angle required\n",
    "#|2012-12-26|2012-12-26 13:45:00|1356551100|              15.0|               360.0|\n",
    "#|2012-12-26|2012-12-26 13:50:00|1356551400|              18.0|               365.5|\n",
    "#|2012-12-26|2012-12-26 13:55:00|1356551700|              21.0|                11.0|\n",
    "\n",
    "#|2012-12-26|2012-12-26 16:05:00|1356559500|              22.0|               360.0|\n",
    "#|2012-12-26|2012-12-26 16:10:00|1356559800|              22.0|               371.0|\n",
    "#|2012-12-26|2012-12-26 16:15:00|1356560100|              22.0|                22.0|\n",
    "\n",
    "#|2012-12-26|2012-12-26 17:45:00|1356565500|              21.0|               360.0|\n",
    "#|2012-12-26|2012-12-26 17:50:00|1356565800|              21.5|               362.5|\n",
    "#|2012-12-26|2012-12-26 17:55:00|1356566100|              22.0|                 5.0|\n",
    "\n",
    "\"\"\"\n",
    "# For reference this is a querry that fixes the degrees that are greater than 360 and less than 0\n",
    "interDF \\\n",
    "    .selectExpr(\"TS\",\"interWindDirectionDegrees\",\"WindDirectionDegrees\",\"nextWindDirectionDegrees as next\",\"oldNextWindDeg\") \\\n",
    "    .where((col(\"interWindDirectionDegrees\") < lit(5)) | (col(\"interWindDirectionDegrees\") > lit(355))) \\\n",
    "    .withColumn(\"X\",expr(\"pmod(interWindDirectionDegrees,360)\")) \\\n",
    "    .show(1000)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Check around 2012-12-26\n",
    "#     \"pmod(interWindDirectionDegrees,360) as WindDirectionDegrees\"\n",
    "windCleanDF = interDF.selectExpr(\n",
    "    \"Date\",\n",
    "    \"cast(interTSEpochSec as timestamp) as TS\",\n",
    "    \"interTSEpochSec as TSEpochSec\",\n",
    "    \"interWindSpeedMPH as WindSpeedMPH\",\n",
    "    \"pmod(interWindDirectionDegrees,360) as WindDirectionDegrees\"\n",
    ")\n",
    "windCleanDF.show(1000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daylight Readings Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayligthWindDF = windCleanDF\n",
    "#dayligthWindDF.createOrReplaceTempView(\"dayligthWindTable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find The Windy Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholdTrans(df,windThreshold, timeThreshold) :\n",
    "    df.where(col(\"WindSpeedMPH\")>windThreshold)\n",
    "\n",
    "#dayligthWindDF.withColumn(\"\")createOrReplaceTempView(\"dayligthWindTable\")\n",
    "\n",
    "# Helper Variables:\n",
    "\n",
    "#TIME_THRESHOLD = TWO_HOURS_IN_5MINS\n",
    "\n",
    "\n",
    "#FIVE_MINUTES_IN_SECS\n",
    "\n",
    "#spark.sql(f\"\"\"\n",
    "#SELECT *, ROW_NUMBER() OVER (PARTITION BY )\n",
    "#FROM dayligthWindTable\n",
    "#\"\"\").show()\n",
    "\n",
    "thresholdTrans(dayligthWindDF,15,TWO_HOURS_IN_5MINS).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
